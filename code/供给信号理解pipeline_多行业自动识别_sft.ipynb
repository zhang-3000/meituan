{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# 供给信号理解Pipeline - 多行业自动识别版本\n",
    "本Notebook支持自动识别数据中的category列，为每个行业动态生成对应的prompt并进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:50:24.972679Z",
     "iopub.status.busy": "2026-01-06T11:50:24.972131Z",
     "iopub.status.idle": "2026-01-06T11:50:24.975951Z",
     "shell.execute_reply": "2026-01-06T11:50:24.975380Z",
     "shell.execute_reply.started": "2026-01-06T11:50:24.972654Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from utils import friday_api, own_model_api\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "# 全局配置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "global_config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:57:53.526761Z",
     "iopub.status.busy": "2026-01-06T11:57:53.526412Z",
     "iopub.status.idle": "2026-01-06T11:57:53.530276Z",
     "shell.execute_reply": "2026-01-06T11:57:53.529733Z",
     "shell.execute_reply.started": "2026-01-06T11:57:53.526739Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据集路径\n",
    "DATA_PATH = \"../data/供给信号文本挖掘评测集V2_for_review_updated.xlsx\"\n",
    "\n",
    "# Friday API 配置\n",
    "formatted_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "tenantId = \"433017463651568\"\n",
    "model_id = \"12969\"\n",
    "model_version = '2'\n",
    "misId = 'zhangyuntao06'\n",
    "model_name = 'Qwen3-30B-A3B-Instruct-2507-Meituan'  # 使用新的Qwen3模型\n",
    "gpu_team = 'hmart-generalshop'\n",
    "gpu_queue = 'root.zw05_training_cluster.hadoop-generalshop.gpu'\n",
    "gpu_type = 'A100-80G'\n",
    "gpu_num = 2\n",
    "batch_size = 4096\n",
    "max_new_tokens = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industry_config_header",
   "metadata": {},
   "source": [
    "# 行业特定prompt参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "industry_config",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:50:28.704406Z",
     "iopub.status.busy": "2026-01-06T11:50:28.703751Z",
     "iopub.status.idle": "2026-01-06T11:50:28.711078Z",
     "shell.execute_reply": "2026-01-06T11:50:28.710570Z",
     "shell.execute_reply.started": "2026-01-06T11:50:28.704383Z"
    }
   },
   "outputs": [],
   "source": [
    "# 行业服务范畴定义\n",
    "HANGYE_FANCHOU = {\n",
    "    \"美甲\": \"涉及服务包含1）美甲服务：本甲上色、指甲延长，款式设计等；2）基础护理：甲面抛光，指甲修型等。注意美睫相关服务不属于美甲行业。\",\n",
    "    \"健身中心\": \"涉及服务包含1）提供健身场所；2）课程/培训；3）健身效果。注意，健身运动后的相关物品，衣着等不属于行业范畴。\",\n",
    "    \"台球\": \"涉及服务包含1）场馆服务：提供场地设备；2）专业服务：助教/陪练服务等。\",\n",
    "    \"保姆\": \"涉及服务包含1）家庭家务服务：家电清洗，衣物洗涤等；2）母婴护理服务：产妇护理，新生儿护理等；3）养老护理服务：健康护理，精神陪伴等；4）病患陪护服务：医院陪护等。不包含针对物品的介绍。\",\n",
    "    \"运动培训\": \"仅指各项运动培训服务中的课程内容、教学服务和教学场地。知识科普等不在范畴内。\",\n",
    "    \"洗浴\": \"涉及服务包含1）桑拿洗浴服务：泡汤，按摩；2）提供场地设施内容：汗蒸房、娱乐休闲区。\"\n",
    "}\n",
    "\n",
    "# 行业CPV属性字典\n",
    "CPV_DICT = {\n",
    "    \"健身中心\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **健身器材**：指具体的设备/器械名称本身\n",
    "   - 包括：跑步机、史密斯架、蝴蝶机、哑铃、杠铃、划船机、椭圆机、龙门架、卧推架、腿举机、筋膜枪等\n",
    "   - 不包括：使用器械的动作名称\n",
    "2. **课程类别**：指训练方式、训练动作、课程类型的完整名称\n",
    "   - 包括：力量训练、有氧训练、瑜伽、普拉提、HIIT、卧推、深蹲、硬拉、肩部训练、背部训练、帕梅拉、健身操、古法健身操等\n",
    "   - 提取规则：保持完整的训练方式/动作名称，不要拆解。包含\"训练\"、\"操\"、\"课\"等的具体名称都应提取。\n",
    "3. **锻炼部位**：仅指身体部位名称\n",
    "   - 包括：全身、臀部、核心、背部、腿部、肩部、胸部、手臂、腰部、腹部等\n",
    "4. **授课形式**：一对一私教、一对二私教、团课、线上直播课、训练营等\n",
    "5. **场馆设施**：泳池、有氧区、力量区、淋浴间、私教房等\n",
    "6. **场馆环境**：每日消毒、空气净化、室内隔音好、超大面积场馆、独立私教房、落地窗、智能门禁等\n",
    "7. **场馆类型**：无人自助健身房、学生健身房、女性专属健身房、普拉提馆、瑜伽馆、私教工作室等\n",
    "8. **门店/器材品牌**：威尔士、一兆韦德、乐刻运动、超级猩猩、KEEP、中田健身、Technogym、匹克等\n",
    "9. **营业时间**：全天可用、早间特惠时段、深夜时段等\n",
    "10. **是否需要预约**：需要预约、不需要预约、仅团课需预约等\n",
    "11. **收费模式**：次卡、月卡、季卡、年卡、按次付费等\n",
    "12. **服务内容**：体质测试、InBody测试、饮食指导、体脂率检测等\n",
    "13. **附加权益**：赠送体验课、饮食指导、亲友体验券、免费提供洗浴用品等\n",
    "14. **教练资质**：ACE认证、NSCA认证、ACSM认证等\n",
    "15. **教练经验**：10年以上教学经验、资深教学经验等\n",
    "16. **教练性别**：美女教练、男教练等\"\"\",\n",
    "    \n",
    "    \"台球\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **培训课程**：零基础入门、进阶技术提升、斯诺克专项、考级培训等\n",
    "2. **赛事支持**：相关赛事信息\n",
    "3. **设备器材**：进口球桌、高端球杆、美式台球桌、私人球杆柜、自动摆球机、计分显示屏等\n",
    "4. **环境设施**：独立包厢、禁烟区、有酒吧、VIP包房等\n",
    "5. **环境风格**：美式复古、现代简约、赛博朋克风、网红风等\n",
    "6. **场馆环境**：室内隔音好、室内宽敞明亮、超大面积场馆、全面禁烟等\n",
    "7. **场馆类型**：无人自助球馆、无烟球馆、智能门店等\n",
    "8. **球杆配置**：品牌公杆、碳素公杆、定期维护/更换皮头等\n",
    "9. **球桌配置**：赛台(金腿/银腿)、钢库桌、TV台(比赛专用)、加热台(斯诺克必备)等\n",
    "10. **球杆/球桌品牌**：Predator(美洲豹)、Mezz(美兹)、Fury(威利)、野豹、环球、乔氏球桌、星牌球桌、Brunswick(宾士域)、Diamond(钻石)等\n",
    "11. **营业时间**：24小时营业、夜间等\n",
    "12. **是否需要预约**：需要预约、不需要预约等\n",
    "13. **收费模式**：小时计费、按局计费、畅打套餐等\n",
    "14. **服务内容**：按小时计费、助教陪练等\n",
    "15. **教练资质**：台协认证教练、国家级教练、省级运动员等\n",
    "16. **助教陪练**：美女陪练、专业陪练、初级助教、资深助教、明星助教、职业选手陪练等\"\"\",\n",
    "    \n",
    "    \"美甲\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **款式风格**：纯色、跳色、法式(传统/变体)、猫眼(晶石/宽猫)、渐变/晕染、腮红甲、新中式、Y2K/辣妹风、老钱风/极简、ins风、手绘款等\n",
    "2. **甲型**：方形、方圆形、杏仁甲、梯形甲/芭蕾甲、尖甲等\n",
    "3. **品牌**：Presto、VETRO、Ageha、国产胶等\n",
    "4. **颜色色系**：红色、裸色系(透色/妈生甲)、美拉德色系(大地色)、多巴胺色系(高饱和)、莫兰迪色系、显白深色系、金属色系、极光粉、魔镜粉等\n",
    "5. **甲油胶质地**：实色胶、透色胶、冰透/果冻、爆闪/亮片、铂金等\n",
    "6. **延长方式**：贴片延长(全贴/半贴)、纸托光疗延长、水晶延长、琉璃延长等\n",
    "7. **装饰元素/饰品**：施华洛世奇钻、锆石、珍珠、金属链条、蝴蝶结等\n",
    "8. **图案设计**：棋盘格、波点、3D堆胶、水波纹、浮雕等\n",
    "9. **封层/光泽度**：超亮封层、磨砂封层等\n",
    "10. **热门IP**：Hello Kitty、美少女战士、疯狂动物城、鬼灭之刃等\n",
    "11. **门店环境**：ins网红装修、私密性强等\n",
    "12. **设备设施**：美甲躺椅(电动沙发)、独立包间、影视区、宠物友好、新风系统等\n",
    "13. **技师等级**：初级美甲师、资深美甲师、店长/技术总监等\n",
    "14. **服务项目**：本甲护理、本甲上色等\n",
    "15. **服务部位**：手部、足部等\n",
    "16. **附加服务**：修眉、手部去角质、手部SPA、足部去死皮/脚后跟护理、嵌甲矫正等\n",
    "17. **适合场景**：通勤、婚礼、旅行、海边、度假、新年、圣诞等\"\"\",\n",
    "    \n",
    "    \"运动培训\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **场地设施**：国际标准比赛场地、专业运动木地板、塑胶/地胶场地、室外草坪、室内恒温泳池、露天场地等\n",
    "2. **场馆环境**：独立淋浴间、干湿分离更衣室、休息区等\n",
    "3. **课程类型**：启蒙兴趣班、基础技能班、体验课/试听课、考证班、寒暑假训练营等\n",
    "4. **教学内容**：篮球、足球、羽毛球、游泳、滑雪等\n",
    "5. **授课形式**：私教、小班、大班等\n",
    "6. **教练资质**：国家二级运动员、退役职业运动员、国际认证(如ACE/NSCA/ITF等)、救生员证/急救证等\n",
    "7. **培训目标**：兴趣培养、体能增强、技能掌握、减肥塑形、升学体育考试、职业考证、运动康复等\n",
    "8. **教学风格**：实战导向、趣味互动型、启发式教学等\n",
    "9. **培训课时**：10节、20节等\"\"\",\n",
    "    \n",
    "    \"洗浴\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **功能区域**：公共休息大厅、胶囊房、榻榻米、观影区、游戏区、全天候自助餐、水果畅吃等\n",
    "2. **装修风格**：现代简约、日式原木等\n",
    "3. **汤泉种类**：碳酸汤、绢丝浴、牛奶浴、室外露天汤泉、日式缸浴、儿童戏水池、按摩池、冷水池、盐浴、药浴、桑拿房、芬兰干蒸等\n",
    "4. **洗护用品**：国际大牌洗护、全套一次性用品、提供高档护肤品/爽肤水、提供化妆品、戴森吹风机等\n",
    "5. **服务项目**：搓背、足疗/足道、中式推拿、精油SPA、采耳、修脚、拔罐/刮痧、醋搓、过夜服务等\n",
    "6. **适用场景**：情侣约会、家庭聚会、团建聚会等\"\"\",\n",
    "    \n",
    "    \"保姆\": \"\"\"**F属性分类判定规则**（重要）：\n",
    "1. **人员类型**：住家保姆、月嫂、育儿嫂等\n",
    "2. **人员年龄**：25-35岁、35-45岁、45-50岁、50岁以上等\n",
    "3. **资质证书**：母婴护理师证、育婴师证、保育员证、健康证、驾驶证、营养师证等\n",
    "4. **擅长菜系**：家常菜、川湘菜、粤菜、面食、西餐简餐、月子餐、宝宝辅食等\n",
    "5. **核心技能**：早教互动、产后康复指导、催乳按摩、小儿推拿、收纳整理、宠物照料等\n",
    "6. **工作内容**：新生儿护理、产妇护理、保洁等\n",
    "7. **从业经验**：1-3年、3-5年、10年以上等\n",
    "8. **学历水平**：小学、初中、高中/中专、大专及以上、本科等\n",
    "9. **服务模式**：24小时住家、白班(早出晚归)、住家、月嫂、做六休一、仅工作日等\n",
    "10. **性格特点**：活泼开朗、温和有耐心、话少干练、喜欢宠物、不怕猫狗等\n",
    "11. **健康状况**：幽门螺杆菌阴性、乙肝五项正常、每年常规体检等\n",
    "12. **服务对象**：新生儿、早产儿、产妇等\"\"\"\n",
    "}\n",
    "\n",
    "# FAB属性分类指引（通用）\n",
    "FAB_GUIDANCE = \"\"\"**F属性（客观特征）**：\n",
    "- 定义：严格按照上述F属性分类判定规则进行挖掘\n",
    "- 核心判断：客观属性（无主观评价），属性值为文本原文\n",
    "\n",
    "**A属性（主观优点）**：\n",
    "- 定义：对客观事物的主观感受和评价性描述\n",
    "- 判断标准：形容词或评价性短语，无程度副词（很/超级/巨/特别等）\n",
    "- 例子：\"专业\"、\"性价比高\"、\"环境好\"、\"服务热情\"\n",
    "\n",
    "**B属性（场景利益）**：\n",
    "- 定义：描述能够达成的效果、解决的问题、适用的场景或人群\n",
    "- 具体分类：\n",
    "  * 人群/身份标签：指适用的具体人群或身份\n",
    "  * 场景：指适用的具体情境或场合\n",
    "  * 目的/效果：指能达成的具体效果或解决的问题\n",
    "- 例子:\"适合初学者\"、\"缓解压力\"、\"亲子互动\"、\"商务宴请\"等\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt_gen_header",
   "metadata": {},
   "source": [
    "# Prompt生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "prompt_generation",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:50:31.832584Z",
     "iopub.status.busy": "2026-01-06T11:50:31.831630Z",
     "iopub.status.idle": "2026-01-06T11:50:31.837429Z",
     "shell.execute_reply": "2026-01-06T11:50:31.836894Z",
     "shell.execute_reply.started": "2026-01-06T11:50:31.832561Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_prompt(content_text, hangye):\n",
    "    \"\"\"为指定行业生成FAB挖掘的prompt\"\"\"\n",
    "    \n",
    "    # 获取行业相关配置\n",
    "    hangye_scope = HANGYE_FANCHOU.get(hangye, \"\")\n",
    "    cpv_attributes = CPV_DICT.get(hangye, \"\")\n",
    "    \n",
    "    prompt = f\"\"\"# 你的角色\n",
    "你是本地生活行业的专家，有着丰富的供给理解知识。你需要基于FAB模型理论，从文本内容中挖掘{hangye}行业下的FAB属性。\n",
    "\n",
    "# FAB模型介绍\n",
    "F（特征/Feature）：客观属性。是产品具备的客观物理属性，是人们对商品进行辨识的信息因素，比如产品的品牌、材料、工艺、尺寸、颜色等。\n",
    "A（优点/Advantage）：主观属性。将客观物理属性提炼为产品优点或者作用，不同用户对于该属性的解读拥有千人千面的主观解读。\n",
    "B（利益/Benefit）：主观属性。阐释产品能够满足的具体场景下的需求，可以理解为某种用处，适用于某种场景/人群。\n",
    "\n",
    "# 核心任务\n",
    "从文本中提取符合{hangye}行业标准的FAB属性，并输出为JSON格式。行业范畴：{hangye_scope}\n",
    "\n",
    "# 行业客观属性及分类规则\n",
    "{cpv_attributes}\n",
    "\n",
    "# FAB属性分类指引\n",
    "{FAB_GUIDANCE}\n",
    "\n",
    "# 关键判别规则（必须遵守）\n",
    "1. **有效性过滤**：\n",
    "   - 舍弃宽泛无意义词汇（如：有效、好、不错、舒服、棒）\n",
    "   - 舍弃程度副词（如：超级、特别、很、巨、极其、十分、比较、更、最）\n",
    "   - 舍弃负向/吐槽内容\n",
    "2. **结构与内容规范**：\n",
    "   - **A/B属性（主观）**：需提炼为规范短语（动宾结构或名词短语），要求连续、非口语化。\n",
    "   - **F属性（客观）**：必须截取原文，且需包含明确主体。\n",
    "3. **格式约束**：\n",
    "   - 禁止包含任何标点符号。\n",
    "   - 单个属性值长度必须小于8个字。\n",
    "\n",
    "# 输出格式\n",
    "你的输出格式为JSON格式，严格按照示例格式输出，其中reasoning字段表示你的推理过程，推理过程需要严格按照任务顺序进行推理并得出最终结果；result为FAB属性提取的结果，result字段是一个list，list中的每个元素是一个dict，一个dict对应不同的FAB属性，type字段表示属性类型，attribute表示对应的值。\n",
    "**重要**：当分类为F时，你需要提取相应的属性类型和属性值，对于同一属性类型的多个属性值，请使用列表格式，如{{\"课程类别\": [\"瑜伽\", \"普拉提\"]}}。当分类为A或B时，如提取到多个属性词，则用','分割，如果没有对应的分类的属性，则attribute字段值为\"无\"。再次提醒，F属性值为文本原文，A/B属性为提炼后的短语。\n",
    "示例输出：\n",
    "{{\"reasoning\": \"推理过程\", \"result\": [{{\"type\": \"F\", \"attribute\": {{\"属性类别\": [\"属性值1\", \"属性值2\"]}}}}, {{\"type\": \"A\", \"attribute\": \"文本1,文本2\"}}, {{\"type\": \"B\", \"attribute\": \"无\"}}]}}\n",
    "\n",
    "# 待处理文本\n",
    "文本内容：{content_text}\n",
    "输出：\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data_load_header",
   "metadata": {},
   "source": [
    "# 加载数据并识别行业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "load_data",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:50:41.676753Z",
     "iopub.status.busy": "2026-01-06T11:50:41.676424Z",
     "iopub.status.idle": "2026-01-06T11:50:41.794597Z",
     "shell.execute_reply": "2026-01-06T11:50:41.794045Z",
     "shell.execute_reply.started": "2026-01-06T11:50:41.676727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载数据：../data/供给信号文本挖掘评测集V2_for_review_updated.xlsx\n",
      "✓ 数据加载成功，共 200 条记录\n",
      "✓ 列名：['category', 'content', 'F', 'A', 'B']\n",
      "\n",
      "✓ 检测到 6 个行业：['保姆', '美甲', '台球', '洗浴', '运动培训', '健身中心']\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "print(f\"正在加载数据：{DATA_PATH}\")\n",
    "data = pd.read_excel(DATA_PATH).fillna('')\n",
    "print(f\"✓ 数据加载成功，共 {len(data)} 条记录\")\n",
    "print(f\"✓ 列名：{list(data.columns)}\")\n",
    "\n",
    "# 检测category列\n",
    "if 'category' not in data.columns:\n",
    "    print(\"✗ 错误：数据中不存在 'category' 列\")\n",
    "    raise ValueError(\"数据中必须包含 'category' 列\")\n",
    "\n",
    "# 识别所有行业\n",
    "industries = data['category'].unique()\n",
    "print(f\"\\n✓ 检测到 {len(industries)} 个行业：{list(industries)}\")\n",
    "\n",
    "# 检查是否所有行业都有配置\n",
    "unconfigured_industries = [ind for ind in industries if ind not in HANGYE_FANCHOU]\n",
    "if unconfigured_industries:\n",
    "    print(f\"⚠ 警告：以下行业未在配置中：{unconfigured_industries}\")\n",
    "    print(\"这些行业将被跳过\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inference_header",
   "metadata": {},
   "source": [
    "# 生成统一的推理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "inference_unified",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:50:51.890809Z",
     "iopub.status.busy": "2026-01-06T11:50:51.890272Z",
     "iopub.status.idle": "2026-01-06T11:50:51.940672Z",
     "shell.execute_reply": "2026-01-06T11:50:51.940192Z",
     "shell.execute_reply.started": "2026-01-06T11:50:51.890786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "开始生成所有行业的推理数据...\n",
      "======================================================================\n",
      "\n",
      "正在处理 保姆：30 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成保姆数据: 100%|██████████| 30/30 [00:00<00:00, 28155.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 美甲：30 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成美甲数据: 100%|██████████| 30/30 [00:00<00:00, 22180.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 台球：30 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成台球数据: 100%|██████████| 30/30 [00:00<00:00, 24523.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 洗浴：30 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成洗浴数据: 100%|██████████| 30/30 [00:00<00:00, 24828.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 运动培训：30 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成运动培训数据: 100%|██████████| 30/30 [00:00<00:00, 30541.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理 健身中心：50 条数据\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  生成健身中心数据: 100%|██████████| 50/50 [00:00<00:00, 31799.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "正在保存统一的JSON文件...\n",
      "✓ JSON文件已保存：all_industries_xhs_zhukeguan_wajue_20260106.json\n",
      "✓ 总共生成 200 条推理数据\n",
      "\n",
      "各行业数据统计：\n",
      "  - 保姆: 30 条\n",
      "  - 美甲: 30 条\n",
      "  - 台球: 30 条\n",
      "  - 洗浴: 30 条\n",
      "  - 运动培训: 30 条\n",
      "  - 健身中心: 50 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 生成所有行业的推理数据到一个JSON文件\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"开始生成所有行业的推理数据...\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "all_friday_samples = []\n",
    "industry_stats = {}\n",
    "\n",
    "for hangye in industries:\n",
    "    # 跳过未配置的行业\n",
    "    if hangye not in HANGYE_FANCHOU:\n",
    "        print(f\"⚠ 跳过未配置的行业：{hangye}\")\n",
    "        continue\n",
    "    \n",
    "    # 过滤该行业的数据\n",
    "    industry_data = data[data['category'] == hangye].reset_index(drop=True)\n",
    "    data_count = len(industry_data)\n",
    "    industry_stats[hangye] = data_count\n",
    "    \n",
    "    print(f\"正在处理 {hangye}：{data_count} 条数据\")\n",
    "    \n",
    "    # 为该行业的每条数据生成prompt\n",
    "    for i in tqdm(range(len(industry_data)), desc=f\"  生成{hangye}数据\"):\n",
    "        content = industry_data.iloc[i, 1]  # content列是第1列\n",
    "        prompt = generate_prompt(content, hangye)\n",
    "        \n",
    "        friday_sample_line_dict = {}\n",
    "        friday_sample_line_dict['input'] = [\n",
    "            {\"role\": \"system\", \"content\": f\"你是一个专业的本地生活行业FAB属性挖掘专家，专注于{hangye}行业。请严格遵守行业特定规则，并在reasoning中体现判断过程。\"},\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "        friday_sample_line_dict['target'] = ''\n",
    "        all_friday_samples.append(friday_sample_line_dict)\n",
    "\n",
    "# 保存统一的JSON文件\n",
    "json_file = f\"all_industries_xhs_zhukeguan_wajue_{formatted_date}.json\"\n",
    "print(f\"\\n正在保存统一的JSON文件...\")\n",
    "with open(json_file, mode='w', encoding='utf-8') as f:\n",
    "    for sample in all_friday_samples:\n",
    "        # ensure_ascii=False可以保证写到文件里的中文不会变成类似这样的编码\\u4ef7\\u503c98\n",
    "        json.dump(sample, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"✓ JSON文件已保存：{json_file}\")\n",
    "print(f\"✓ 总共生成 {len(all_friday_samples)} 条推理数据\")\n",
    "print(f\"\\n各行业数据统计：\")\n",
    "for hangye, count in industry_stats.items():\n",
    "    print(f\"  - {hangye}: {count} 条\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f843063c-3e85-43e2-8791-c5f9a968f011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:58:02.397321Z",
     "iopub.status.busy": "2026-01-06T11:58:02.396947Z",
     "iopub.status.idle": "2026-01-06T11:58:05.101311Z",
     "shell.execute_reply": "2026-01-06T11:58:05.100398Z",
     "shell.execute_reply.started": "2026-01-06T11:58:02.397298Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "正在上传到HDFS...\n",
      "✓ 文件已上传到HDFS\n"
     ]
    }
   ],
   "source": [
    "# 上传到HDFS\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"正在上传到HDFS...\")\n",
    "!hdfs dfs -put -f $json_file viewfs://hadoop-meituan/user/hadoop-hmart-generalshop/yaoyu13/xhs/\n",
    "print(f\"✓ 文件已上传到HDFS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c459c49d-5810-4af1-853f-da7e8b1cc590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-06T11:58:08.599801Z",
     "iopub.status.busy": "2026-01-06T11:58:08.599277Z",
     "iopub.status.idle": "2026-01-06T11:58:10.196488Z",
     "shell.execute_reply": "2026-01-06T11:58:10.195904Z",
     "shell.execute_reply.started": "2026-01-06T11:58:08.599774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "正在调用Friday API进行统一推理...\n",
      "{\"code\":0,\"data\":321404,\"message\":\"成功\"}\n",
      "\n",
      "======================================================================\n",
      "✓✓✓ 推理任务已提交！✓✓✓\n",
      "======================================================================\n",
      "\n",
      "任务名称: xhs_all_industries_prompt_task_sft_V2\n",
      "数据文件: all_industries_xhs_zhukeguan_wajue_20260106.json\n",
      "总数据量: 200 条\n",
      "\n",
      "请到 Friday 平台查看任务状态，完成后记录任务ID用于下载结果。\n"
     ]
    }
   ],
   "source": [
    "# 调用Friday API（只调用一次）\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"正在调用Friday API进行统一推理...\")\n",
    "task_name = f\"xhs_all_industries_prompt_task_sft_V2\"\n",
    "response = own_model_api(\n",
    "    task_name=task_name,\n",
    "    tenantId=tenantId,\n",
    "    json_file=f'viewfs://hadoop-meituan/user/hadoop-hmart-generalshop/yaoyu13/xhs/{json_file}',\n",
    "    misId=misId,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    gpu_team=gpu_team,\n",
    "    gpu_queue=gpu_queue,\n",
    "    gpu_type=gpu_type,\n",
    "    gpu_num=gpu_num,\n",
    "    batch_size=batch_size,\n",
    "    max_new_tokens=max_new_tokens\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"✓✓✓ 推理任务已提交！✓✓✓\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n任务名称: {task_name}\")\n",
    "print(f\"数据文件: {json_file}\")\n",
    "print(f\"总数据量: {len(all_friday_samples)} 条\")\n",
    "print(f\"\\n请到 Friday 平台查看任务状态，完成后记录任务ID用于下载结果。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "result_header",
   "metadata": {},
   "source": [
    "# 下载并解析结果\n",
    "⚠️ 注意：需要等待Friday API推理完成后再执行此部分\n",
    "\n",
    "你可以在Friday平台查看任务状态，完成后记录任务ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "download_results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T03:43:37.978704Z",
     "iopub.status.busy": "2026-01-07T03:43:37.978320Z",
     "iopub.status.idle": "2026-01-07T03:43:40.540585Z",
     "shell.execute_reply": "2026-01-07T03:43:40.539871Z",
     "shell.execute_reply.started": "2026-01-07T03:43:37.978670Z"
    }
   },
   "outputs": [],
   "source": [
    "# 跟据结果地址下载json文件\n",
    "!hdfs dfs -get viewfs://hadoop-meituan/user/hadoop-hmart-generalshop/friday/eval_out/321622_xhs_all_industries_prompt_task_sft_V3.json ./infer_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parse_header",
   "metadata": {},
   "source": [
    "# 解析结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "parse_results",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T03:45:13.146120Z",
     "iopub.status.busy": "2026-01-07T03:45:13.145742Z",
     "iopub.status.idle": "2026-01-07T03:45:13.303192Z",
     "shell.execute_reply": "2026-01-07T03:45:13.302403Z",
     "shell.execute_reply.started": "2026-01-07T03:45:13.146097Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 19781.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解析内容失败数量：0/200\n",
      "结果已保存到: ./results/3_hangye_sft_V3.xlsx\n",
      "\n",
      "前5行预览:\n",
      "  category                                            content  F  A  B  \\\n",
      "0       保姆  52岁阿姨的陆冲向前Pumping。第一次户外上课，一镜到底的向前Pumping。教练说现在...  无  无  无   \n",
      "1       保姆  阿姨自己买的不给吃？。#家政改变女人的生活 #月嫂育婴师 #迷茫与成长 #月嫂育婴师养老护理...  无  无  无   \n",
      "2       保姆  搬家不头疼！大家电搬运保姆级攻略。很多小伙伴在问：“大家电搬家怎么搬才不会翻车？”别慌！今天...  无  无  无   \n",
      "3       保姆  保洁阿姨可是精得很呀！早知道收拾好再退房。#令人想笑的offer #搞笑 #反转 #vlog...  无  无  无   \n",
      "4       保姆  避雷烟台月嫂YBY! !。烟台芝罘区月嫂姚××在我家干了八天半我忍无可忍让她走了，后悔没早点...  无  无  无   \n",
      "\n",
      "                                              pred_F          pred_A  \\\n",
      "0                                                                  无   \n",
      "1                  人员类型:'月嫂',人员类型: '育婴师',服务内容:'养老护理'               无   \n",
      "2  服务内容:'大家电搬运',服务内容: '精细打包',服务内容: '还原整理',服务模式:'全...  服务透明,精细打包,专业团队   \n",
      "3                                                                  无   \n",
      "4  服务模式:'24小时住家',服务模式: '白班',人员类型:'月嫂',工作内容:'新生儿护理...               无   \n",
      "\n",
      "              pred_B                                          reasoning  \n",
      "0                  无  首先分析文本内容，发现主要描述的是52岁阿姨参与陆地冲浪板教学，进行户外上课和Pumping...  \n",
      "1          居家养老,母婴护理  首先分析文本内容，发现主要涉及家政、月嫂、育婴师、养老护理等关键词。根据行业F属性分类规则，...  \n",
      "2  安全搬家,无忧搬家,适合大家电用户  首先分析文本内容，聚焦于保姆行业相关的服务供给。文本主要围绕搬家服务展开，涉及大家电搬运、精...  \n",
      "3                  无  首先分析文本内容，主要描述了保洁阿姨的行为，但没有具体提及保姆行业的服务内容、人员类型、资质...  \n",
      "4         适合新生儿,适合产妇  首先对文本进行有效性过滤，去除负向评价和无意义词汇。分析文本内容，提取与保姆行业相关的客观属...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 文本挖掘\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 读取原始标注数据\n",
    "origin_data = pd.read_excel(\"../data/供给信号文本挖掘评测集V2_for_review_updated.xlsx\")\n",
    "\n",
    "# 读取模型推理结果\n",
    "file = open(\"/home/sankuai/dolphinfs_zhangyuntao06/daily_January/1.6/code/infer_results/321622_xhs_all_industries_prompt_task_sft_V3.json\", 'r', encoding='utf-8')\n",
    "\n",
    "deal_result = []\n",
    "for line in tqdm(file.readlines()):\n",
    "    dic = json.loads(line)\n",
    "    deal_result.append(dic)\n",
    "\n",
    "pred_list = []\n",
    "reasoning_list = []\n",
    "count = 0\n",
    "for i in range(len(deal_result)):\n",
    "    response = deal_result[i]['output']['content']\n",
    "    if \"```json\" in response:\n",
    "        json_str = response.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"{\" in response and \"}\" in response:\n",
    "        start = response.find(\"{\")\n",
    "        end = response.rfind(\"}\") + 1\n",
    "        json_str = response[start:end]\n",
    "    else:\n",
    "        json_str = response\n",
    "    try:\n",
    "        # 提取F属性\n",
    "        f_attr_str = \"\"\n",
    "        try:\n",
    "            f_match = re.search(r'\"type\"\\s*:\\s*\"F\".*?\"attribute\"\\s*:\\s*({[^}]+})', json_str, re.DOTALL)\n",
    "            if f_match:\n",
    "                f_attr_dict_str = eval(f_match.group(1))\n",
    "                for p,v in f_attr_dict_str.items():\n",
    "                    v_list = str(v).replace(\"[\",\"\").replace(\"]\",\"\").split(\",\")\n",
    "                    for v in v_list:\n",
    "                        f_attr_str += p + \":\" + v + \",\"\n",
    "        except:\n",
    "            count += 1\n",
    "            f_attr_str = \"无\"\n",
    "    \n",
    "        # 提取A和B属性\n",
    "        a_result = re.findall(r'\"type\":\"A\",\"attribute\":\"(.*?)\"', json_str.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        b_result = re.findall(r'\"type\":\"B\",\"attribute\":\"(.*?)\"', json_str.replace(\" \",\"\").replace(\"\\n\",\"\"))\n",
    "        a_attr = a_result[-1] if len(a_result) > 0 else \"无\"\n",
    "        b_attr = b_result[-1] if len(b_result) > 0 else \"无\"\n",
    "\n",
    "        # 提取reasoning字段（模型推理过程）\n",
    "        try:\n",
    "            reasoning_match = re.search(r'\"reasoning\"\\s*:\\s*\"(.*?)\"(?:,|$)', json_str, re.DOTALL)\n",
    "            if reasoning_match:\n",
    "                reasoning_text = reasoning_match.group(1)\n",
    "                # 处理转义字符\n",
    "                reasoning_text = reasoning_text.replace('\\\\n', '\\n').replace('\\\\\"', '\"')\n",
    "                reasoning_list.append(reasoning_text)\n",
    "            else:\n",
    "                reasoning_list.append(\"\")\n",
    "        except:\n",
    "            reasoning_list.append(\"\")\n",
    "\n",
    "        pred_list.append([f_attr_str.rstrip(\",\"), a_attr, b_attr])\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "    except:\n",
    "        count += 1\n",
    "        pred_list.append([\"无\", \"无\", \"无\"])\n",
    "        reasoning_list.append(\"\")\n",
    "            \n",
    "# 添加预测结果列\n",
    "origin_data['pred_F'] = [item[0] for item in pred_list]\n",
    "origin_data['pred_A'] = [item[1] for item in pred_list]\n",
    "origin_data['pred_B'] = [item[2] for item in pred_list]\n",
    "origin_data['reasoning'] = reasoning_list\n",
    " \n",
    "# 保存为xlsx文件\n",
    "output_file = \"./results/3_hangye_sft_V3.xlsx\"\n",
    "origin_data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"解析内容失败数量：{count}/{len(deal_result)}\")\n",
    "print(f\"结果已保存到: {output_file}\")\n",
    "print(f\"\\n前5行预览:\")\n",
    "print(origin_data[['category', 'content', 'F', 'A', 'B', 'pred_F', 'pred_A', 'pred_B', 'reasoning']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad8b951-d7e1-4152-82f9-880f316612f6",
   "metadata": {},
   "source": [
    "# 过滤模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "993de945-7d92-44ad-bf12-555c7409bf18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-07T03:46:00.125949Z",
     "iopub.status.busy": "2026-01-07T03:46:00.125406Z",
     "iopub.status.idle": "2026-01-07T03:46:01.543591Z",
     "shell.execute_reply": "2026-01-07T03:46:01.542812Z",
     "shell.execute_reply.started": "2026-01-07T03:46:00.125927Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "过滤模块\n",
      "================================================================================\n",
      "✓ 地名过滤器已加载，共 34907 个地名\n",
      "✓ 副词过滤器已加载，共 38 个副词\n",
      "✓ 泛词过滤器已加载，共 18 个泛词\n",
      "\n",
      "读取解析结果文件...\n",
      "✓ 已读取 200 条记录\n",
      "\n",
      "✓ 过滤完成，结果已保存到: ./results/3_hangye_sft_V3_filtered.xlsx\n",
      "\n",
      "【F属性 (特征)】\n",
      "  过滤词数: 11\n",
      "  过滤最多的3个词: 日本胶, 马卡龙美甲, 新加坡桑拿\n",
      "\n",
      "【A属性 (优点)】\n",
      "  过滤词数: 6\n",
      "  过滤最多的3个词: 宝宝护理很专业, 手法非常娴熟, 服务很顶\n",
      "\n",
      "【B属性 (利益)】\n",
      "  过滤词数: 6\n",
      "  过滤最多的3个词: 放松解压好去处, 下班放松好去处, 新加坡周末好去处输出\n"
     ]
    }
   ],
   "source": [
    "filter_input_file = \"./results/3_hangye_sft_V3.xlsx\"\n",
    "filter_output_file = \"./results/3_hangye_sft_V3_filtered.xlsx\"\n",
    "\n",
    "def parse_f_attribute_string(f_str):\n",
    "    \"\"\"\n",
    "    解析 F 属性（字符串格式 - 用于人工标注数据）\n",
    "\n",
    "    Args:\n",
    "        f_str: F 属性字符串\n",
    "\n",
    "    Returns:\n",
    "        属性值列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not f_str or f_str == '无' or f_str == '':\n",
    "            return []\n",
    "        f_str = str(f_str).strip()\n",
    "        items = [item.strip() for item in f_str.replace('，', ',').split(',')]\n",
    "        result = []\n",
    "        for item in items:\n",
    "            if item and item != '无':\n",
    "                if '：' in item:\n",
    "                    result.append(item.split('：')[1].strip())\n",
    "                elif ':' in item:\n",
    "                    result.append(item.split(':')[1].strip())\n",
    "                else:\n",
    "                    result.append(item)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def parse_ab_attribute(ab_str):\n",
    "    \"\"\"\n",
    "    解析 A/B 属性（逗号分隔格式）\n",
    "\n",
    "    Args:\n",
    "        ab_str: A/B 属性字符串\n",
    "\n",
    "    Returns:\n",
    "        属性值列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not ab_str or ab_str == '无' or ab_str == '':\n",
    "            return []\n",
    "        ab_str = str(ab_str).replace('，', ',')\n",
    "        return [item.strip() for item in ab_str.split(',') if item.strip()]\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "def parse_pred_f_attribute(f_str):\n",
    "    \"\"\"\n",
    "    解析预测的 F 属性（冒号分隔格式）\n",
    "\n",
    "    Args:\n",
    "        f_str: F 属性字符串（格式：key:value,key:value,...）\n",
    "\n",
    "    Returns:\n",
    "        属性值列表\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not f_str or f_str == '无' or f_str == '':\n",
    "            return []\n",
    "        f_str = str(f_str).strip()\n",
    "        items = [item.strip() for item in f_str.split(',')]\n",
    "        result = []\n",
    "        for item in items:\n",
    "            if item and item != '无':\n",
    "                if ':' in item:\n",
    "                    result.append(item.split(':')[1].strip())\n",
    "                else:\n",
    "                    result.append(item)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return []\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"过滤模块\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 初始化过滤规则\n",
    "filters = []\n",
    "\n",
    "# 1. 过滤地名（如果文件存在）\n",
    "try:\n",
    "    address_file = \"../data/地点全集.xlsx\"\n",
    "    if os.path.exists(address_file):\n",
    "        address = pd.read_excel(address_file, sheet_name=\"Sheet1\")\n",
    "        address_list = address['address'].tolist()\n",
    "        if address_list:\n",
    "            address_filter = '|'.join(address_list)\n",
    "            filters.append(address_filter)\n",
    "            print(f\"✓ 地名过滤器已加载，共 {len(address_list)} 个地名\")\n",
    "except:\n",
    "    print(\"⚠ 地名过滤器加载失败或文件不存在\")\n",
    "\n",
    "# 2. 过滤比较副词\n",
    "adv = ['很', '非常', '极了', '较', '极其', '有点', '极端', '极度', '特别', '十分',\n",
    "       '格外', '尤其', '极为', '超级', '异常', '无比', '绝对', '完全', '稍微', '稍稍',\n",
    "       '稍许', '多少', '略微', '还算', '微微', '一点', '一些', '几乎', '差不多', '简直',\n",
    "       '近乎', '差点', '差一点', '彻底', '毫不', '丝毫', '根本', '全然']\n",
    "adv_filter = '|'.join(adv)\n",
    "filters.append(adv_filter)\n",
    "print(f\"✓ 副词过滤器已加载，共 {len(adv)} 个副词\")\n",
    "\n",
    "# 3. 过滤通用泛词\n",
    "common = [\n",
    "    \"运动健身\", \"好的教练\", \"有效\", \"好\", \"不错\", \"舒服\", \"棒\",\n",
    "    \"很好\", \"特别好\", \"超级好\", \"挺好\", \"还不错\", \"还可以\",\n",
    "    \"效果好\", \"效果不错\", \"效果很好\", \"不错的\", \"很不错\"\n",
    "]\n",
    "common_filter = '|'.join(common)\n",
    "filters.append(common_filter)\n",
    "print(f\"✓ 泛词过滤器已加载，共 {len(common)} 个泛词\")\n",
    "\n",
    "# 合并所有过滤器\n",
    "all_filter = '|'.join(filters)\n",
    "\n",
    "# 读取解析模块的输出文件\n",
    "print(\"\\n读取解析结果文件...\")\n",
    "\n",
    "filter_data = pd.read_excel(filter_input_file)\n",
    "print(f\"✓ 已读取 {len(filter_data)} 条记录\")\n",
    "\n",
    "# 统计过滤结果\n",
    "from collections import Counter\n",
    "\n",
    "removed_f_words = []  # 被移除/过滤掉的词\n",
    "removed_a_words = []\n",
    "removed_b_words = []\n",
    "\n",
    "for idx in range(len(filter_data)):\n",
    "    label_f_list = parse_f_attribute_string(filter_data.iloc[idx]['F'])\n",
    "    label_a_list = parse_ab_attribute(filter_data.iloc[idx]['A'])\n",
    "    label_b_list = parse_ab_attribute(filter_data.iloc[idx]['B'])\n",
    "\n",
    "    # 收集所有被移除的词（匹配过滤规则的词）\n",
    "    for word in label_f_list:\n",
    "        if re.search(all_filter, word):\n",
    "            removed_f_words.append(word)\n",
    "\n",
    "    for word in label_a_list:\n",
    "        if re.search(all_filter, word):\n",
    "            removed_a_words.append(word)\n",
    "\n",
    "    for word in label_b_list:\n",
    "        if re.search(all_filter, word):\n",
    "            removed_b_words.append(word)\n",
    "\n",
    "# 统计被移除的词汇频次\n",
    "f_counter = Counter(removed_f_words)\n",
    "a_counter = Counter(removed_a_words)\n",
    "b_counter = Counter(removed_b_words)\n",
    "\n",
    "# 获取过滤词数\n",
    "f_filtered_count = len(removed_f_words)\n",
    "a_filtered_count = len(removed_a_words)\n",
    "b_filtered_count = len(removed_b_words)\n",
    "\n",
    "# 生成过滤后的文件（保留基础列）\n",
    "\n",
    "filter_data.to_excel(filter_output_file, index=False)\n",
    "\n",
    "# 输出统计信息\n",
    "print(f\"\\n✓ 过滤完成，结果已保存到: {filter_output_file}\\n\")\n",
    "\n",
    "# F属性统计 - 显示被过滤掉的最多的3个词\n",
    "print(f\"【F属性 (特征)】\")\n",
    "print(f\"  过滤词数: {f_filtered_count}\")\n",
    "top_3_f = f_counter.most_common(3)\n",
    "print(f\"  过滤最多的3个词: {', '.join([f'{word}' for word, count in top_3_f])}\")\n",
    "\n",
    "# A属性统计 - 显示被过滤掉的最多的3个词\n",
    "print(f\"\\n【A属性 (优点)】\")\n",
    "print(f\"  过滤词数: {a_filtered_count}\")\n",
    "top_3_a = a_counter.most_common(3)\n",
    "print(f\"  过滤最多的3个词: {', '.join([f'{word}' for word, count in top_3_a])}\")\n",
    "\n",
    "# B属性统计 - 显示被过滤掉的最多的3个词\n",
    "print(f\"\\n【B属性 (利益)】\")\n",
    "print(f\"  过滤词数: {b_filtered_count}\")\n",
    "top_3_b = b_counter.most_common(3)\n",
    "print(f\"  过滤最多的3个词: {', '.join([f'{word}' for word, count in top_3_b])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e0c819-b4e4-42c4-b48f-d1ad16439201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
