#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
FABå±æ€§è¯„æµ‹è„šæœ¬ V2 - ä¸¤é˜¶æ®µè¯„æµ‹
ç¬¬ä¸€é˜¶æ®µï¼šè°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­ï¼Œä¿å­˜æ‰€æœ‰LLMåˆ¤æ–­ç»“æœ
ç¬¬äºŒé˜¶æ®µï¼šè¯»å–ä¿å­˜çš„LLMç»“æœï¼Œè®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡ã€åˆ†ç±»å‡†ç¡®ç‡
"""

import pandas as pd
import json
import os
import time
from openai import OpenAI
from tqdm import tqdm
from datetime import datetime

# ============================================================================
# é…ç½®åŒºåŸŸ
# ============================================================================

# è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆè¿‡æ»¤æ¨¡å—è¾“å‡ºçš„æ–‡ä»¶ï¼‰
INPUT_FILE_PATH = "/home/sankuai/dolphinfs_zhangyuntao06/daily_January/1.6/code/results/3_hangye_sft_V3_filtered.xlsx"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = "../results/evaluation"

# è¾“å‡ºæ–‡ä»¶å‰ç¼€
OUTPUT_PREFIX = "fab_evaluation_3_hangye"

# æŒ‡å®šéœ€è¦ç»Ÿè®¡çš„è¡Œä¸š
TARGET_INDUSTRIES = ["å¥èº«ä¸­å¿ƒ", "å°çƒ", "è¿åŠ¨åŸ¹è®­"]

# æ˜¯å¦è·³è¿‡ç¬¬ä¸€é˜¶æ®µï¼ˆä»…ä½¿ç”¨å·²ä¿å­˜çš„LLMç»“æœè®¡ç®—æŒ‡æ ‡ï¼‰
SKIP_LLM_STAGE = False

# ============================================================================

# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯
client = OpenAI(
    api_key="1871844672277114930",
    base_url="https://aigc.sankuai.com/v1/openai/native"
)

# é¢‘ç‡æ§åˆ¶
last_request_time = 0
request_interval = 1.0

# LLM è¯„åˆ¤ prompt
JUDGE_PROMPT = """ä½ æ˜¯ä¸€åç®—æ³•ä¸“å®¶ï¼Œä½ éœ€è¦å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
1. åˆ¤å®šæ¨¡å‹é¢„æµ‹çš„ç»“æœæ˜¯å¦æœ‰ä¸äººå·¥æ ‡æ³¨çš„ç»“æœä¸­å­˜åœ¨ç›¸åŒçš„å«ä¹‰çš„çŸ­è¯­ï¼Œè‹¥å­˜åœ¨ç›¸åŒå«ä¹‰ç»“æœåˆ™è¾“å‡º"æ˜¯"ï¼Œå¦åˆ™è¾“å‡º"å¦"ã€‚
2. åˆ¤å®šæ¨¡å‹é¢„æµ‹çš„ç»“æœæ˜¯ä¸»è§‚å±æ€§è¿˜æ˜¯å®¢è§‚å±æ€§ï¼Œè‹¥ä¸ºå®¢è§‚å±æ€§åˆ™è¾“å‡º"å®¢è§‚"ï¼Œè‹¥ä¸ºä¸»è§‚å±æ€§åˆ™è¾“å‡º"ä¸»è§‚"ã€‚

ã€ä¸»å®¢è§‚å±æ€§å®šä¹‰ã€‘
1. å®¢è§‚å±æ€§ï¼šæ˜¯æŒ‡äº§å“å…·å¤‡çš„å®¢è§‚ç‰©ç†å±æ€§ï¼Œæ˜¯äººä»¬å¯¹å•†å“è¿›è¡Œè¾¨è¯†çš„ä¿¡æ¯å› ç´ ï¼Œæ¯”å¦‚äº§å“çš„å“ç‰Œã€ææ–™ã€å·¥è‰ºã€å°ºå¯¸ã€é¢œè‰²ç­‰ã€‚
2. ä¸»è§‚å±æ€§ï¼šæ˜¯å°†å®¢è§‚ç‰©ç†å±æ€§æç‚¼ä¸ºäº§å“ä¼˜ç‚¹æˆ–è€…ä½œç”¨ï¼Œä¸åŒç”¨æˆ·å¯¹äºè¯¥å±æ€§çš„è§£è¯»æ‹¥æœ‰åƒäººåƒé¢çš„ä¸»è§‚è§£è¯»ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
è¾“å‡ºä¸ºJSONæ ¼å¼ï¼Œæ ¼å¼ä¸º{"judge_1": "æ˜¯/å¦", "judge_2": "ä¸»è§‚/å®¢è§‚"}

ä¸‹é¢å¼€å§‹æ‰§è¡Œä»»åŠ¡ï¼š
æ¨¡å‹é¢„æµ‹ç»“æœï¼š%s
äººå·¥æ ‡æ³¨ç»“æœï¼š%s
è¾“å‡ºï¼š
"""


def llm_check(predict_text, label_text, model="gpt-4.1", max_retries=10):
    """
    è°ƒç”¨ LLM è¿›è¡Œç›¸ä¼¼åº¦å’Œå±æ€§åˆ†ç±»åˆ¤å®š
    å¦‚æœé‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œä¼šè‡ªåŠ¨ç­‰å¾…åé‡è¯•

    Args:
        predict_text: æ¨¡å‹é¢„æµ‹ç»“æœ
        label_text: äººå·¥æ ‡æ³¨ç»“æœ
        model: ä½¿ç”¨çš„æ¨¡å‹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

    Returns:
        LLM çš„åˆ¤å®šç»“æœ (JSONå­—ç¬¦ä¸²)
    """
    global last_request_time

    for retry in range(max_retries):
        try:
            # é¢‘ç‡æ§åˆ¶
            current_time = time.time()
            time_since_last = current_time - last_request_time
            if time_since_last < request_interval:
                sleep_time = request_interval - time_since_last
                time.sleep(sleep_time)

            last_request_time = time.time()

            prompt = JUDGE_PROMPT % (predict_text, label_text)
            result = client.chat.completions.create(
                model=model,
                messages=[
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                stream=False
            )
            return result.choices[0].message.content
        except Exception as e:
            error_msg = str(e)

            if retry < max_retries - 1:
                # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
                if '429' in error_msg or 'é¢‘ç‡' in error_msg or 'é™åˆ¶' in error_msg:
                    wait_time = 2  # é€Ÿç‡é™åˆ¶æ—¶ç­‰å¾…2ç§’
                    tqdm.write(f"âš ï¸  ã€é€Ÿç‡é™åˆ¶ã€‘ç­‰å¾… {wait_time}s åé‡è¯•... (ç¬¬ {retry + 1}/{max_retries} æ¬¡)")
                else:
                    wait_time = 2   # å…¶ä»–é”™è¯¯ç­‰å¾…2ç§’
                    tqdm.write(f"âš ï¸  ã€è°ƒç”¨å¤±è´¥ã€‘ç­‰å¾… {wait_time}s åé‡è¯•... (ç¬¬ {retry + 1}/{max_retries} æ¬¡)")

                time.sleep(wait_time)
            else:
                tqdm.write(f"âœ— ã€è°ƒç”¨å¤±è´¥ã€‘è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•° ({max_retries})")
                return 'error'


def parse_llm_result(llm_response):
    """
    è§£æ LLM çš„è¿”å›ç»“æœ

    Args:
        llm_response: LLM è¿”å›çš„å­—ç¬¦ä¸²

    Returns:
        (judge_1, judge_2) å…ƒç»„ï¼Œå…¶ä¸­ judge_1 ä¸º"æ˜¯"æˆ–"å¦"ï¼Œjudge_2 ä¸º"ä¸»è§‚"æˆ–"å®¢è§‚"
    """
    try:
        if llm_response == 'error' or not llm_response:
            return None, None

        # å°è¯•è§£æ JSON
        if '{' in llm_response and '}' in llm_response:
            json_str = llm_response[llm_response.find('{'):llm_response.rfind('}')+1]
            result = json.loads(json_str)
            judge_1 = result.get('judge_1', '')
            judge_2 = result.get('judge_2', '')
            return judge_1, judge_2
        else:
            # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥æ£€æŸ¥å…³é”®è¯
            judge_1 = 'æ˜¯' if 'æ˜¯' in llm_response else 'å¦'
            judge_2 = 'ä¸»è§‚' if 'ä¸»è§‚' in llm_response else 'å®¢è§‚'
            return judge_1, judge_2
    except Exception as e:
        return None, None


def parse_f_attribute_string(f_str):
    """
    è§£æ F å±æ€§ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ - ç”¨äºäººå·¥æ ‡æ³¨æ•°æ®ï¼‰

    Args:
        f_str: F å±æ€§å­—ç¬¦ä¸²

    Returns:
        å±æ€§å€¼åˆ—è¡¨
    """
    try:
        if pd.isna(f_str) or not f_str or f_str == 'æ— ' or f_str == '' or f_str == 'nan':
            return []
        f_str = str(f_str).strip()
        items = [item.strip() for item in f_str.replace('ï¼Œ', ',').split(',')]
        result = []
        for item in items:
            if item and item != 'æ— ' and item != 'nan':
                if 'ï¼š' in item:
                    result.append(item.split('ï¼š')[1].strip())
                elif ':' in item:
                    result.append(item.split(':')[1].strip())
                else:
                    result.append(item)
        return result
    except Exception as e:
        return []


def parse_pred_f_attribute(f_str):
    """
    è§£æé¢„æµ‹çš„ F å±æ€§ï¼ˆå†’å·åˆ†éš”æ ¼å¼ï¼‰

    Args:
        f_str: F å±æ€§å­—ç¬¦ä¸²ï¼ˆæ ¼å¼ï¼škey:value,key:value,...ï¼‰

    Returns:
        å±æ€§å€¼åˆ—è¡¨
    """
    try:
        if pd.isna(f_str) or not f_str or f_str == 'æ— ' or f_str == '' or f_str == 'nan':
            return []
        f_str = str(f_str).strip()
        items = [item.strip() for item in f_str.replace('ï¼Œ', ',').split(',')]
        result = []
        for item in items:
            if item and item != 'æ— ' and item != 'nan':
                if ':' in item:
                    result.append(item.split(':')[1].strip())
                else:
                    result.append(item)
        return result
    except Exception as e:
        return []


def parse_ab_attribute(ab_str):
    """
    è§£æ A/B å±æ€§ï¼ˆé€—å·åˆ†éš”æ ¼å¼ï¼‰

    Args:
        ab_str: A/B å±æ€§å­—ç¬¦ä¸²

    Returns:
        å±æ€§å€¼åˆ—è¡¨
    """
    try:
        if pd.isna(ab_str) or not ab_str or ab_str == 'æ— ' or ab_str == '' or ab_str == 'nan':
            return []
        ab_str = str(ab_str).replace('ï¼Œ', ',')
        return [item.strip() for item in ab_str.split(',') if item.strip() and item.strip() != 'nan']
    except Exception as e:
        return []


def load_data():
    """åŠ è½½æ•°æ®æ–‡ä»¶"""
    print("\nåŠ è½½è¯„æµ‹æ•°æ®...")
    print("-" * 80)

    if not INPUT_FILE_PATH:
        print("âœ— é”™è¯¯: INPUT_FILE_PATH æœªæŒ‡å®š")
        return None

    if not os.path.exists(INPUT_FILE_PATH):
        print(f"âœ— é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨")
        print(f"  æŒ‡å®šçš„è·¯å¾„: {INPUT_FILE_PATH}")
        return None

    try:
        print(f"åŠ è½½æ–‡ä»¶: {INPUT_FILE_PATH}")
        data = pd.read_excel(INPUT_FILE_PATH)
        print(f"âœ“ æ•°æ®åŠ è½½æˆåŠŸï¼Œå…± {len(data)} æ¡è®°å½•")
        print(f"åˆ—å: {list(data.columns)}")
        return data
    except Exception as e:
        print(f"âœ— æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return None


def stage_1_llm_judgment(data):
    """
    ç¬¬ä¸€é˜¶æ®µï¼šè°ƒç”¨LLMè¿›è¡Œåˆ¤æ–­ï¼Œä¿å­˜æ‰€æœ‰ç»“æœ
    å•æ¡æ•°æ®å¤±è´¥æ—¶ï¼Œåœ¨åŸåœ°ç­‰å¾…åé‡å‘ï¼Œç›´åˆ°æˆåŠŸæ‰å¤„ç†ä¸‹ä¸€æ¡
    """
    print("\n" + "=" * 80)
    print("ã€ç¬¬ä¸€é˜¶æ®µã€‘LLM åˆ¤æ–­é˜¶æ®µï¼ˆå•æ¡æ•°æ®æœ¬åœ°é‡è¯•ï¼‰")
    print("=" * 80)
    print("\nåˆå§‹åŒ–LLMåˆ¤æ–­åˆ—...")
    print("-" * 80)

    # åˆå§‹åŒ–æ–°åˆ—ç”¨äºä¿å­˜LLMç»“æœ
    data['llm_judge_results'] = ''  # ä¿å­˜æ‰€æœ‰LLMè°ƒç”¨çš„ç»“æœ
    data['llm_call_count'] = 0      # è®°å½•æ¯è¡Œè°ƒç”¨LLMçš„æ¬¡æ•°

    llm_call_total = 0  # å…¨å±€LLMè°ƒç”¨æ¬¡æ•°

    print(f"âœ“ å·²åˆå§‹åŒ–LLMåˆ¤æ–­åˆ—")

    print("\nå¼€å§‹LLMåˆ¤æ–­...")
    print("-" * 80)

    for idx in tqdm(range(len(data)), desc="LLMåˆ¤æ–­è¿›åº¦"):
        # è·å–è¡Œä¸šä¿¡æ¯
        industry = str(data.iloc[idx].get('category', 'æœªçŸ¥')).strip()

        # åªå¤„ç†æŒ‡å®šè¡Œä¸šçš„æ•°æ®
        if industry not in TARGET_INDUSTRIES:
            continue

        # è§£ææ ‡æ³¨æ•°æ®ï¼ˆFã€Aã€Bï¼‰
        label_f_list = parse_f_attribute_string(data.iloc[idx]['F'])
        label_a_list = parse_ab_attribute(data.iloc[idx]['A'])
        label_b_list = parse_ab_attribute(data.iloc[idx]['B'])

        # è§£æé¢„æµ‹æ•°æ®ï¼ˆpred_Fã€pred_Aã€pred_Bï¼‰
        predict_f_list = parse_pred_f_attribute(data.iloc[idx]['pred_F'])
        predict_a_list = parse_ab_attribute(data.iloc[idx]['pred_A'])
        predict_b_list = parse_ab_attribute(data.iloc[idx]['pred_B'])

        # å¦‚æœæ ‡æ³¨å’Œé¢„æµ‹éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡è¿™ä¸€è¡Œ
        if (not label_f_list and not label_a_list and not label_b_list and
            not predict_f_list and not predict_a_list and not predict_b_list):
            continue

        # ç»„åˆæ ‡æ³¨å’Œé¢„æµ‹çš„ä¸»å®¢è§‚å±æ€§
        zhuguan_label_list = label_a_list + label_b_list
        keguan_label_list = label_f_list
        zhuguan_predict_list = predict_a_list + predict_b_list
        keguan_predict_list = predict_f_list

        llm_results = []  # ä¿å­˜æœ¬è¡Œçš„æ‰€æœ‰LLMç»“æœ
        llm_call_count = 0

        # å¤„ç†é¢„æµ‹çš„ A å±æ€§
        for a in predict_a_list:
            if a not in zhuguan_label_list and a not in keguan_label_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(a, str(zhuguan_label_list + keguan_label_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'predict_a',
                    'value': a,
                    'llm_response': res
                })

        # å¤„ç†é¢„æµ‹çš„ B å±æ€§
        for b in predict_b_list:
            if b not in zhuguan_label_list and b not in keguan_label_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(b, str(zhuguan_label_list + keguan_label_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'predict_b',
                    'value': b,
                    'llm_response': res
                })

        # å¤„ç†é¢„æµ‹çš„ F å±æ€§
        for f in predict_f_list:
            if f not in zhuguan_label_list and f not in keguan_label_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(f, str(zhuguan_label_list + keguan_label_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'predict_f',
                    'value': f,
                    'llm_response': res
                })

        # å¤„ç†æ ‡æ³¨çš„ A å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for a in label_a_list:
            if a not in zhuguan_predict_list and a not in keguan_predict_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(a, str(zhuguan_predict_list + keguan_predict_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'label_a',
                    'value': a,
                    'llm_response': res
                })

        # å¤„ç†æ ‡æ³¨çš„ B å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for b in label_b_list:
            if b not in zhuguan_predict_list and b not in keguan_predict_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(b, str(zhuguan_predict_list + keguan_predict_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'label_b',
                    'value': b,
                    'llm_response': res
                })

        # å¤„ç†æ ‡æ³¨çš„ F å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for f in label_f_list:
            if f not in zhuguan_predict_list and f not in keguan_predict_list:
                # éœ€è¦LLMåˆ¤å®š
                res = llm_check(f, str(zhuguan_predict_list + keguan_predict_list))
                llm_call_count += 1
                llm_call_total += 1
                llm_results.append({
                    'type': 'label_f',
                    'value': f,
                    'llm_response': res
                })

        # ä¿å­˜æœ¬è¡Œçš„LLMç»“æœ
        data.at[idx, 'llm_judge_results'] = json.dumps(llm_results, ensure_ascii=False)
        data.at[idx, 'llm_call_count'] = llm_call_count

    print("\n" + "=" * 80)
    print(f"âœ“ LLMåˆ¤æ–­å®Œæˆï¼")
    print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {llm_call_total}")
    print("=" * 80)

    return data, llm_call_total


def stage_2_calculate_metrics(data, llm_call_total):
    """
    ç¬¬äºŒé˜¶æ®µï¼šè¯»å–ä¿å­˜çš„LLMç»“æœï¼Œè®¡ç®—å„é¡¹æŒ‡æ ‡
    """
    print("\n" + "=" * 80)
    print("ã€ç¬¬äºŒé˜¶æ®µã€‘æŒ‡æ ‡è®¡ç®—é˜¶æ®µ")
    print("=" * 80)

    print("\nåˆå§‹åŒ–è¯„æµ‹åˆ—...")
    print("-" * 80)

    data['precision_error'] = ''
    data['recall_error'] = ''
    data['classification_error'] = ''

    print(f"âœ“ å·²åˆå§‹åŒ–è¯„æµ‹åˆ—")

    print("\nå¼€å§‹è®¡ç®—æŒ‡æ ‡...")
    print("-" * 80)

    # å…¨å±€è®¡æ•°å™¨
    jingque_c = 0        # ç²¾ç¡®ç‡åˆ†å­ï¼šæ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ•°é‡
    jingque_c_all = 0    # ç²¾ç¡®ç‡åˆ†æ¯ï¼šæ¨¡å‹é¢„æµ‹çš„æ€»æ•°é‡

    zhaohui_c = 0        # å¬å›ç‡åˆ†å­ï¼šæ¨¡å‹é¢„æµ‹æ­£ç¡®çš„æ•°é‡
    zhaohui_c_all = 0    # å¬å›ç‡åˆ†æ¯ï¼šäººå·¥æ ‡æ³¨çš„æ€»æ•°é‡

    fenlei_c = 0         # åˆ†ç±»å‡†ç¡®ç‡åˆ†å­ï¼šåˆ†ç±»æ­£ç¡®çš„æ•°é‡
    fenlei_c_all = 0     # åˆ†ç±»å‡†ç¡®ç‡åˆ†æ¯ï¼šæ¨¡å‹é¢„æµ‹çš„æ€»æ•°é‡

    # æŒ‰è¡Œä¸šç»Ÿè®¡çš„å­—å…¸
    industry_stats = {}

    for idx in tqdm(range(len(data)), desc="æŒ‡æ ‡è®¡ç®—è¿›åº¦"):
        jingque_error_list = []
        zhaohui_error_list = []
        fenlei_error_list = []

        # è·å–è¡Œä¸šä¿¡æ¯
        industry = str(data.iloc[idx].get('category', 'æœªçŸ¥')).strip()

        # åªå¤„ç†æŒ‡å®šè¡Œä¸šçš„æ•°æ®
        if industry not in TARGET_INDUSTRIES:
            continue

        if industry not in industry_stats:
            industry_stats[industry] = {
                'jingque_c': 0,
                'jingque_c_all': 0,
                'zhaohui_c': 0,
                'zhaohui_c_all': 0,
                'fenlei_c': 0,
                'fenlei_c_all': 0,
                'count': 0
            }
        industry_stats[industry]['count'] += 1

        # è®°å½•æœ¬è¡Œå¼€å§‹æ—¶çš„è®¡æ•°å€¼
        row_jingque_c_start = jingque_c
        row_zhaohui_c_start = zhaohui_c
        row_fenlei_c_start = fenlei_c
        row_jingque_c_all_start = jingque_c_all
        row_zhaohui_c_all_start = zhaohui_c_all
        row_fenlei_c_all_start = fenlei_c_all

        # è§£ææ ‡æ³¨æ•°æ®ï¼ˆFã€Aã€Bï¼‰
        label_f_list = parse_f_attribute_string(data.iloc[idx]['F'])
        label_a_list = parse_ab_attribute(data.iloc[idx]['A'])
        label_b_list = parse_ab_attribute(data.iloc[idx]['B'])

        # è§£æé¢„æµ‹æ•°æ®ï¼ˆpred_Fã€pred_Aã€pred_Bï¼‰
        predict_f_list = parse_pred_f_attribute(data.iloc[idx]['pred_F'])
        predict_a_list = parse_ab_attribute(data.iloc[idx]['pred_A'])
        predict_b_list = parse_ab_attribute(data.iloc[idx]['pred_B'])

        # å¦‚æœæ ‡æ³¨å’Œé¢„æµ‹éƒ½ä¸ºç©ºï¼Œåˆ™è·³è¿‡è¿™ä¸€è¡Œ
        if (not label_f_list and not label_a_list and not label_b_list and
            not predict_f_list and not predict_a_list and not predict_b_list):
            continue

        # ç»„åˆæ ‡æ³¨å’Œé¢„æµ‹çš„ä¸»å®¢è§‚å±æ€§
        zhuguan_label_list = label_a_list + label_b_list
        keguan_label_list = label_f_list
        zhuguan_predict_list = predict_a_list + predict_b_list
        keguan_predict_list = predict_f_list

        # åŠ è½½æœ¬è¡Œçš„LLMç»“æœ
        llm_results_json = data.iloc[idx]['llm_judge_results']
        llm_results = json.loads(llm_results_json) if llm_results_json else []
        llm_results_dict = {(r['type'], r['value']): r['llm_response'] for r in llm_results}

        # ===== è®¡ç®—ç²¾ç¡®ç‡ =====
        # æ£€æŸ¥é¢„æµ‹çš„ A å±æ€§ï¼ˆç²¾ç¡®ç‡ï¼‰
        for a in predict_a_list:
            if a in zhuguan_label_list:
                jingque_c += 1
                fenlei_c += 1
            elif a in keguan_label_list:
                jingque_c += 1
                fenlei_error_list.append(f"a,{a}")
            else:
                # ä»å·²ä¿å­˜çš„LLMç»“æœä¸­è·å–
                res = llm_results_dict.get(('predict_a', a), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    jingque_c += 1
                    if 'ä¸»è§‚' in res:
                        fenlei_c += 1
                    else:
                        fenlei_error_list.append(f"a,{a}")
                elif res == 'error':
                    jingque_error_list.append(f"a,{a}")

        # æ£€æŸ¥é¢„æµ‹çš„ B å±æ€§ï¼ˆç²¾ç¡®ç‡ï¼‰
        for b in predict_b_list:
            if b in zhuguan_label_list:
                jingque_c += 1
                fenlei_c += 1
            elif b in keguan_label_list:
                jingque_c += 1
                fenlei_error_list.append(f"b,{b}")
            else:
                res = llm_results_dict.get(('predict_b', b), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    jingque_c += 1
                    if 'ä¸»è§‚' in res:
                        fenlei_c += 1
                    else:
                        fenlei_error_list.append(f"b,{b}")
                elif res == 'error':
                    jingque_error_list.append(f"b,{b}")

        # æ£€æŸ¥é¢„æµ‹çš„ F å±æ€§ï¼ˆç²¾ç¡®ç‡ï¼‰
        for f in predict_f_list:
            if f in zhuguan_label_list:
                jingque_c += 1
                fenlei_error_list.append(f"f,{f}")
            elif f in keguan_label_list:
                jingque_c += 1
                fenlei_c += 1
            else:
                res = llm_results_dict.get(('predict_f', f), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    jingque_c += 1
                    if 'å®¢è§‚' in res:
                        fenlei_c += 1
                    else:
                        fenlei_error_list.append(f"f,{f}")
                elif res == 'error':
                    jingque_error_list.append(f"f,{f}")

        # ç²¾ç¡®ç‡åˆ†æ¯
        predict_total = len(predict_a_list) + len(predict_b_list) + len(predict_f_list)
        jingque_c_all += predict_total
        fenlei_c_all += predict_total

        # ===== è®¡ç®—å¬å›ç‡ =====
        # æ£€æŸ¥æ ‡æ³¨çš„ A å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for a in label_a_list:
            if a in zhuguan_predict_list:
                zhaohui_c += 1
            elif a in keguan_predict_list:
                zhaohui_c += 1
            else:
                res = llm_results_dict.get(('label_a', a), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    zhaohui_c += 1
                else:
                    zhaohui_error_list.append(f"a,{a}")

        # æ£€æŸ¥æ ‡æ³¨çš„ B å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for b in label_b_list:
            if b in zhuguan_predict_list:
                zhaohui_c += 1
            elif b in keguan_predict_list:
                zhaohui_c += 1
            else:
                res = llm_results_dict.get(('label_b', b), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    zhaohui_c += 1
                else:
                    zhaohui_error_list.append(f"b,{b}")

        # æ£€æŸ¥æ ‡æ³¨çš„ F å±æ€§ï¼ˆå¬å›ç‡ï¼‰
        for f in label_f_list:
            if f in zhuguan_predict_list:
                zhaohui_c += 1
            elif f in keguan_predict_list:
                zhaohui_c += 1
            else:
                res = llm_results_dict.get(('label_f', f), 'error')
                if res != 'error' and 'æ˜¯' in res:
                    zhaohui_c += 1
                else:
                    zhaohui_error_list.append(f"f,{f}")

        # å¬å›ç‡åˆ†æ¯
        label_total = len(label_a_list) + len(label_b_list) + len(label_f_list)
        zhaohui_c_all += label_total

        # ä¿å­˜é”™è¯¯ä¿¡æ¯
        data.at[idx, 'precision_error'] = ';'.join(jingque_error_list)
        data.at[idx, 'recall_error'] = ';'.join(zhaohui_error_list)
        data.at[idx, 'classification_error'] = ';'.join(fenlei_error_list)

        # ç»Ÿè®¡è¯¥è¡Œå¯¹è¡Œä¸šçš„è´¡çŒ®
        industry_stats[industry]['jingque_c'] += jingque_c - row_jingque_c_start
        industry_stats[industry]['jingque_c_all'] += jingque_c_all - row_jingque_c_all_start
        industry_stats[industry]['zhaohui_c'] += zhaohui_c - row_zhaohui_c_start
        industry_stats[industry]['zhaohui_c_all'] += zhaohui_c_all - row_zhaohui_c_all_start
        industry_stats[industry]['fenlei_c'] += fenlei_c - row_fenlei_c_start
        industry_stats[industry]['fenlei_c_all'] += fenlei_c_all - row_fenlei_c_all_start

    # 4. è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    print("\n" + "=" * 80)
    print("è¯„æµ‹ç»“æœ")
    print("=" * 80)

    # æŒ‰è¡Œä¸šç»Ÿè®¡ç»“æœ
    print("\nã€æŒ‰è¡Œä¸šç»Ÿè®¡ã€‘")
    print("-" * 80)
    for industry in TARGET_INDUSTRIES:
        if industry in industry_stats:
            stats = industry_stats[industry]
            precision = stats['jingque_c'] / stats['jingque_c_all'] if stats['jingque_c_all'] > 0 else 0
            recall = stats['zhaohui_c'] / stats['zhaohui_c_all'] if stats['zhaohui_c_all'] > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            classification_acc = stats['fenlei_c'] / stats['fenlei_c_all'] if stats['fenlei_c_all'] > 0 else 0

            print(f"\n{industry} (å…± {stats['count']} æ¡è®°å½•)")
            print(f"  ç²¾ç¡®ç‡: {precision:.4f}  ({stats['jingque_c']}/{stats['jingque_c_all']})")
            print(f"  å¬å›ç‡: {recall:.4f}  ({stats['zhaohui_c']}/{stats['zhaohui_c_all']})")
            print(f"  F1åˆ†æ•°: {f1:.4f}")
            print(f"  åˆ†ç±»å‡†ç¡®ç‡: {classification_acc:.4f}  ({stats['fenlei_c']}/{stats['fenlei_c_all']})")

    # å…¨å±€ç»Ÿè®¡ç»“æœï¼ˆä¸‰è¡Œä¸šæ€»ä½“ï¼‰
    print("\n" + "=" * 80)
    print("ã€ä¸‰è¡Œä¸šæ€»ä½“ç»Ÿè®¡ã€‘")
    print("-" * 80)

    precision = jingque_c / jingque_c_all if jingque_c_all > 0 else 0
    recall = zhaohui_c / zhaohui_c_all if zhaohui_c_all > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
    classification_accuracy = fenlei_c / fenlei_c_all if fenlei_c_all > 0 else 0

    print(f"\nã€ç²¾ç¡®ç‡ (Precision)ã€‘")
    print(f"  æ•°å€¼: {precision:.4f}")
    print(f"  è¯¦æƒ…: {jingque_c} / {jingque_c_all}")

    print(f"\nã€å¬å›ç‡ (Recall)ã€‘")
    print(f"  æ•°å€¼: {recall:.4f}")
    print(f"  è¯¦æƒ…: {zhaohui_c} / {zhaohui_c_all}")

    print(f"\nã€F1 åˆ†æ•°ã€‘")
    print(f"  æ•°å€¼: {f1_score:.4f}")

    print(f"\nã€ä¸»å®¢è§‚å±æ€§åˆ†ç±»å‡†ç¡®ç‡ã€‘")
    print(f"  æ•°å€¼: {classification_accuracy:.4f}")
    print(f"  è¯¦æƒ…: {fenlei_c} / {fenlei_c_all}")

    print(f"\nã€LLM è°ƒç”¨ç»Ÿè®¡ã€‘")
    print(f"  æ€»è°ƒç”¨æ¬¡æ•°: {llm_call_total}")

    return data, industry_stats, precision, recall, f1_score, classification_accuracy


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 80)
    print("FABå±æ€§è¯„æµ‹è„šæœ¬ V2 - ä¸¤é˜¶æ®µè¯„æµ‹ï¼ˆä¿å­˜LLMç»“æœï¼‰")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    data = load_data()
    if data is None:
        return

    llm_call_total = 0

    # 2. ç¬¬ä¸€é˜¶æ®µï¼šLLM åˆ¤æ–­
    if not SKIP_LLM_STAGE:
        data, llm_call_total = stage_1_llm_judgment(data)
    else:
        print("\nâ­ï¸  è·³è¿‡ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨å·²ä¿å­˜çš„LLMç»“æœ...")

    # 3. ç¬¬äºŒé˜¶æ®µï¼šè®¡ç®—æŒ‡æ ‡
    data, industry_stats, precision, recall, f1_score, classification_accuracy = stage_2_calculate_metrics(data, llm_call_total)

    # 4. ä¿å­˜è¯„æµ‹ç»“æœ
    print("\n" + "=" * 80)
    print("ä¿å­˜è¯„æµ‹ç»“æœ...")
    print("-" * 80)

    os.makedirs(OUTPUT_DIR, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # 4.1 ä¿å­˜è¯¦ç»†çš„ Excel ç»“æœæ–‡ä»¶
    result_file = os.path.join(OUTPUT_DIR, f"{OUTPUT_PREFIX}_{timestamp}.xlsx")
    data.to_excel(result_file, index=False)
    print(f"âœ“ è¯¦ç»†è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {result_file}")

    # 4.2 ä¿å­˜ TXT æ ¼å¼çš„ç®€æ´æŠ¥å‘Š
    report_file = os.path.join(OUTPUT_DIR, f"{OUTPUT_PREFIX}_{timestamp}.txt")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("FABå±æ€§è¯„æµ‹æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")

        f.write(f"è¯„æµ‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»è®°å½•æ•°: {len(data)}\n")
        f.write(f"LLMè°ƒç”¨æ€»æ¬¡æ•°: {llm_call_total}\n\n")

        # ã€æŒ‰è¡Œä¸šç»Ÿè®¡ã€‘
        f.write("=" * 80 + "\n")
        f.write("ã€æŒ‰è¡Œä¸šç»Ÿè®¡ã€‘\n")
        f.write("=" * 80 + "\n\n")

        for industry in TARGET_INDUSTRIES:
            if industry in industry_stats:
                stats = industry_stats[industry]
                precision_ind = stats['jingque_c'] / stats['jingque_c_all'] if stats['jingque_c_all'] > 0 else 0
                recall_ind = stats['zhaohui_c'] / stats['zhaohui_c_all'] if stats['zhaohui_c_all'] > 0 else 0
                f1_ind = 2 * (precision_ind * recall_ind) / (precision_ind + recall_ind) if (precision_ind + recall_ind) > 0 else 0
                classification_acc_ind = stats['fenlei_c'] / stats['fenlei_c_all'] if stats['fenlei_c_all'] > 0 else 0

                f.write(f"{industry} (å…± {stats['count']} æ¡è®°å½•)\n\n")
                f.write(f"  ç²¾ç¡®ç‡: {precision_ind:.4f}  ({stats['jingque_c']}/{stats['jingque_c_all']})\n")
                f.write(f"  å¬å›ç‡: {recall_ind:.4f}  ({stats['zhaohui_c']}/{stats['zhaohui_c_all']})\n")
                f.write(f"  F1åˆ†æ•°: {f1_ind:.4f}\n")
                f.write(f"  åˆ†ç±»å‡†ç¡®ç‡: {classification_acc_ind:.4f}  ({stats['fenlei_c']}/{stats['fenlei_c_all']})\n\n")

        # ã€ä¸‰è¡Œä¸šæ€»ä½“ç»Ÿè®¡ã€‘
        f.write("=" * 80 + "\n")
        f.write("ã€ä¸‰è¡Œä¸šæ€»ä½“ç»Ÿè®¡ã€‘\n")
        f.write("=" * 80 + "\n\n")

        f.write("ã€ç²¾ç¡®ç‡ (Precision)ã€‘\n")
        f.write(f"  æ•°å€¼: {precision:.4f}\n\n")

        f.write("ã€å¬å›ç‡ (Recall)ã€‘\n")
        f.write(f"  æ•°å€¼: {recall:.4f}\n\n")

        f.write("ã€F1 åˆ†æ•°ã€‘\n")
        f.write(f"  æ•°å€¼: {f1_score:.4f}\n\n")

        f.write("ã€ä¸»å®¢è§‚å±æ€§åˆ†ç±»å‡†ç¡®ç‡ã€‘\n")
        f.write(f"  æ•°å€¼: {classification_accuracy:.4f}\n\n")

        f.write("ã€LLMè°ƒç”¨ç»Ÿè®¡ã€‘\n")
        f.write(f"  æ€»è°ƒç”¨æ¬¡æ•°: {llm_call_total}\n\n")

        f.write("=" * 80 + "\n")

    print(f"âœ“ TXT æ ¼å¼æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

    # 5. æ‰“å°æœ€ç»ˆæ€»ç»“
    print("\n" + "=" * 80)
    print("âœ“âœ“âœ“ è¯„æµ‹å®Œæˆ! âœ“âœ“âœ“")
    print("=" * 80)
    print(f"\nã€è¾“å‡ºæ–‡ä»¶ä½ç½®ã€‘")
    print(f"  ğŸ“Š è¯¦ç»†ç»“æœ: {result_file}")
    print(f"  ğŸ“„ TXT æŠ¥å‘Š: {report_file}")
    print(f"  ğŸ’¾ LLMç»“æœå·²ä¿å­˜åœ¨ Excel çš„ 'llm_judge_results' åˆ—ä¸­")
    print("=" * 80 + "\n")


if __name__ == "__main__":
    main()
